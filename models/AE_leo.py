# -*- coding: utf-8 -*-
"""autoencoder_leo.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YmroOrCa60Uiaxo6ntRAyAiOiE9NKhg3
"""
import torch
import torch.nn as nn


class Autoencoder(nn.Module):
    def __init__(self):
        super().__init__()

        self.encoder = nn.Sequential(
            nn.Linear(28**2, 256),
            nn.ReLU(),
            nn.Linear(256, 64),
            nn.ReLU(),
            nn.Linear(64, 16),
            nn.ReLU()
        )

        self.decoder = nn.Sequential(
            nn.Linear(16, 64),
            nn.ReLU(),
            nn.Linear(64, 256),
            nn.ReLU(),
            nn.Linear(256, 28**2),
            nn.Sigmoid()
        )

    def forward(self, x):
        encoded = self.encoder(x)
        decoded = self.decoder(encoded)
        return decoded


if __name__ == '__main__':
    import torch
    from torchvision import datasets
    from torchvision import transforms
    import matplotlib.pyplot as plt
    import torch.nn as nn
    import torchvision.datasets as datasets
    from torchvision.transforms import ToTensor
    import time
    from torch.autograd import Variable
    import numpy as np
    # from IPython.display import clear_output

    # datasets
    mnist_trainset = datasets.MNIST(
        root='./data', train=True, download=True, transform=ToTensor())
    mnist_testset = datasets.MNIST(
        root='./data', train=False, download=True, transform=ToTensor())

    # holdout
    indexes = []
    for i in range(len(mnist_trainset)):
        if mnist_trainset[i][1] != 1 and mnist_trainset[i][1] != 7:
            indexes.append(i)
    # print(len(indexes)/len(mnist_trainset))

    # torch.utils.data.Subset(trainset, idx)
    mnist_trainset_holdout = torch.utils.data.Subset(
        mnist_trainset, indexes)

    # dataloader
    trainloader = torch.utils.data.DataLoader(
        mnist_trainset_holdout, batch_size=50)
    testloader = torch.utils.data.DataLoader(mnist_testset, batch_size=50)

    model = Autoencoder()
    loss_mse = nn.MSELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)

    def train(epochs):

        t = time.time()
        losses = []
        outputs = []
        model.train()

        for epoch in range(1, epochs+1):
            for i, (images, labels) in enumerate(testloader):
                for image in images:
                    image = image.reshape(-1, 28*28)
                    new_img = model(image)
                    loss = loss_mse(new_img, image)
                    losses.append(loss)

                    optimizer.zero_grad()
                    loss.backward()
                    optimizer.step()
                if i % 50 == 0:
                    print(i)
            outputs.append((epochs, image, new_img))

            print(epoch, time.time()-t)
            t = time.time()

    for i, (images, labels) in enumerate(testloader):
        for image in images:
            img = image
            break
        break

    plt.imshow(img.squeeze())
    print(len(testloader))

    epochs = 5

    train(epochs)

    t = time.time()
    for i, (images, labels) in enumerate(testloader):

        # getting the test images
        for image in images:
            # clear_output()
            fig = plt.figure(figsize=(8, 8))
            image = image.reshape(-1, 28*28)
            new_img = model(image)
            image = image.reshape(-1, 28, 28)
            new_img = new_img.reshape(-1, 28, 28)
            fig.add_subplot(2, 2, 1)
            plt.imshow(image.squeeze())
            fig.add_subplot(2, 2, 2)
            plt.imshow((new_img.cpu().detach().numpy()).squeeze())
            plt.show()
            time.sleep(1)
            if time.time() - t > 30:
                break
        if time.time() - t > 30:
            break

    model = Autoencoder()
    torch.save(model, 'AE_leoB-NA-001.pth')
